\documentclass[aspectratio=169]{beamer}
\usetheme{simple}

\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
% \input{preamble/preamble_acronyms.tex}

\title{STATS271/371: Applied Bayesian Statistics}
\subtitle{Mixed Membership Models, Topic Models, and Variational Inference}
\author{Scott Linderman}
\date{\today}


\begin{document}


\maketitle

\begin{frame}{Box's Loop}
\begin{center}
\includegraphics[width=.85\linewidth]{figures/lap1/boxsloop.jpeg}\\
\end{center} 
\begin{flushright}
{\footnotesize Blei, \textit{Ann. Rev. Stat. App.} 2014.}
\end{flushright}
\end{frame}

\begin{frame}{Lap 5: Mixed Membership Models and Variational Inference}
\begin{itemize}
    \item \hyperref[sec:models]{\textbf{Model:} Mixed Membership Models (specifically, topic models)}
    \item \hyperref[sec:cavi]{\textbf{Algorithm:} Coordinate Ascent Variational Inference (CAVI)}
\end{itemize}

\vspace{3em}

\begin{center}
    {\Large \textcolor{red}{These slides are based on unpublished lecture notes by Dave Blei.}}
\end{center}

\end{frame}


\section{Model: Mixed Membership Models}
\label{sec:models}

\begin{frame}{Mixed Membership Models}
    
    Mixed membership models are designed for \textit{grouped data}.
    
    Each ``data point'' is itself a collection of observations. For example,
    \begin{itemize}
        \item in text analysis, a document is a collection of observed words.
        \item in social science, a survey is a collection of observed answers.
        \item in genetic sequencing, a genome is a collection of observed genes.
    \end{itemize}
    
    Mixed membership models look for patterns like the components of a mixture model, but allowing each data point to involve multiple components.
    
\end{frame}


\begin{frame}{Notation for a mixed membership model}
    
\begin{itemize}
    \item Let $\mbx_n = (x_{n,1}, \ldots, x_{n,L})$ denote $n$-th \textbf{data point}. It contains a \textbf{collection of observations}. To simplify notation, assume all data points are length $L$. 

    \item Each data point reflects a \textbf{combination of $K$ mixture components} with parameters $\{\mbeta_k\}_{k=1}^K$. These are shared by all documents.

    \item Each data point has its own \textbf{mixture proportions}, $\mbpi_n \in \Delta_K$.

    \item Each \textbf{observation is assigned to one of the components} by $z_{n,\ell} \in \{1, \ldots, K\}$.
\end{itemize}

\end{frame}


\begin{frame}{Topic models}

The most common mixed-membership model is the \textbf{topic model}, a generative model for documents.

Topic models are so common, they have their own nomenclature:

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \textit{data set} & \textit{corpus} & collection of documents  \\
        \textit{data point} & \textit{document} & collection of words \\
        \textit{observation} & \textit{word} & one element of a document \\
        \textit{mixture component} & \textit{topic} & distribution over words \\
        \textit{mixture proportions} & \textit{topic proportions} & distribution over topics \\
        \textit{mixture assignment} & \textit{topic assignment} & which topic produced a word \\
    \end{tabular}
    \caption{Rosetta stone for translating between mixed membership model and topic model notation.}
    \label{tab:my_label}
\end{table}
\end{frame}

\begin{frame}{Topic modeling intuition}
\centering
\includegraphics[width=.65\textwidth]{figures/lap5/intuition.png}

From \citet{blei2012probabilistic}.
\end{frame}


\begin{frame}{Topic modeling of New York Times articles}
\centering
\includegraphics[width=.5\textwidth]{figures/lap5/nyt.png}

\end{frame}


\begin{frame}{Dynamic topic modeling of Science articles}
\centering
\includegraphics[width=.75\textwidth]{figures/lap5/dtm.png}

From \citet{blei2006dynamic}.
\end{frame}

\begin{frame}{Topic modeling of congressional voting records}
\centering
\includegraphics[width=.5\textwidth]{figures/lap5/taxes.png}

From \citet{Boyd-Graber2017-qk}.
\end{frame}


% \begin{frame}{Other applications of topic modeling}
    
%     % Survey data
    
%     % Genetics
    
%     % Behavior data
    
% \end{frame}

\begin{frame}{The generative process for a mixed membership model}
\label{slide:gen_mm}
The generative model process is:
\begin{itemize}
    \item for each mixture component $k = 1, \ldots, K$, sample its parameter $\mbeta_k \sim p(\mbeta_k \mid \mbphi)$
    \item for each data point $n=1,\ldots, N$:
    \begin{itemize}
        \item sample mixture proportions $\mbpi_n \sim \distDirichlet(\mbpi_n \mid \mbalpha)$ 
        \item for each observation $\ell = 1, \ldots, L$:
        \begin{itemize}
            \item sample mixture assignment $z_{n,\ell} \sim \mbpi_n$ 
            \item sample observation $x_{n, \ell} \sim p(x \mid \mbeta_{z_{n,\ell}})$
        \end{itemize}
    \end{itemize}
\end{itemize}
The mixed membership model allows sharing at the dataset level (all data points share the same components) while allowing variability at the data point level (each data point has its own mixture proportions).

\end{frame}

\begin{frame}{The generative process for a topic model}
Slide~\ref{slide:gen_mm} in the language of topic modeling: 
\begin{itemize}
    \item for each \textbf{topic} $k = 1, \ldots, K$, sample its parameter $\mbeta_k \sim p(\mbeta_k \mid \mbphi)$
    \item for each \textbf{document} $n=1,\ldots, N$:
    \begin{itemize}
        \item sample \textbf{topic proportions} $\mbpi_n \sim \distDirichlet(\mbpi_n \mid \mbalpha)$ 
        \item for each \textbf{word} $\ell = 1, \ldots, L$:
        \begin{itemize}
            \item sample \textbf{topic assignment} $z_{n,\ell} \sim \mbpi_n$ 
            \item sample \textbf{word} $x_{n, \ell} \sim p(x \mid \mbeta_{z_{n,\ell}})$
        \end{itemize}
    \end{itemize}
\end{itemize}
The topic model captures sharing at the corpus level (all documents share the same topics) while allowing variability at the data point level (each document weights topics differently).

\end{frame}


\begin{frame}{The joint distribution}
    
The joint probability for a general mixed membership model is,
\begin{align}
p(\{\mbeta_k\}_{k=1}^K, \{\mbpi_n, \mbz_n, \mbx_n\}_{n=1}^N \mid \mbphi, \mbalpha) 
&=
\prod_{k=1}^K p(\mbeta_k \mid \mbphi) 
\prod_{n=1}^N \left[ p(\mbpi_n \mid \mbalpha) 
\prod_{\ell=1}^L p(z_{n,\ell} \mid \mbpi_n) \, p(x_{n,\ell} \mid \mbeta_{z_{n,\ell}}) \right]
\end{align}
As in mixture models, we can write this equivalently as 
\begin{align}
p(\{\mbeta_k\}_{k=1}^K, \{\mbpi_n, \mbz_n, \mbx_n\}_{n=1}^N \mid \mbphi, \mbalpha) 
= 
\prod_{k=1}^K p(\mbeta_k \mid \mbphi) 
\prod_{n=1}^N \left[ p(\mbpi_n \mid \mbalpha) 
\prod_{\ell=1}^L \prod_{k=1}^K \left[\pi_{n,k} \, p(x_{n,\ell} \mid \mbeta_{k})\right]^{\bbI[z_{n,\ell} = k]} \right]
\end{align}
\end{frame}


\begin{frame}[t]{Latent Dirichlet allocation}
    
Latent Dirichlet Allocation (LDA) \citep{blei2003latent} is the most widely used topic model. 

It assumes conjugate Dirichlet-Categorical model for the topics $\mbeta_k$ and words $x_{n,\ell}$,
\begin{align}
    p(\mbeta_k \mid \mbphi) &= \distDirichlet(\mbeta_k \mid \mbphi), \\
    % p(\mbpi_n \mid \mbalpha) &= \distDirichlet(\mbpi_n \mid \mbalpha) \\
    p(x_{n, \ell} \mid \mbeta_{z_{n,\ell}}) &= \mbeta_{z_{n, \ell}, x_{n, \ell}}
\end{align}

Plugging in these assumptions, the joint probability is,
\begin{align}
p(\{\mbeta_k\}_{k=1}^K, & \{\mbpi_n, \mbz_n, \mbx_n\}_{n=1}^N \mid \mbphi, \mbalpha) \\
& = 
\label{eq:lda_joint}
\prod_{k=1}^K \distDirichlet(\mbeta_k \mid \mbphi) 
\prod_{n=1}^N \left[ \distDirichlet(\mbpi_n \mid \mbalpha) 
\prod_{\ell=1}^L \pi_{n,z_{n,\ell}} \, \eta_{z_{n,\ell},x_{n,\ell}} \right] \\
% & = 
% \prod_{k=1}^K \distDirichlet(\mbeta_k \mid \mbphi) 
% \prod_{n=1}^N \left[ \distDirichlet(\mbpi_n \mid \mbalpha) 
% \prod_{\ell=1}^L \prod_{k=1}^K \left[\pi_{n,k} \, \eta_{k,x_{n,\ell}} \right]^{\bbI[z_{n,\ell} = k]} \right] \\
& = 
\prod_{k=1}^K \distDirichlet(\mbeta_k \mid \mbphi) 
\prod_{n=1}^N \Bigg( \distDirichlet(\mbpi_n \mid \mbalpha) 
\prod_{\ell=1}^L \left[ \prod_{k=1}^K \pi_{n,k}^{\bbI[z_{n,\ell} = k]} \right] \left[ \prod_{k=1}^K \prod_{v=1}^V \eta_{k,v}^{\bbI[z_{n,\ell}=k] \bbI[x_{n,\ell} = v]} \right] \Bigg)
\end{align}
\end{frame}

\begin{frame}{Gibbs sampling for LDA}
As usual, sample each variable from its conditional distribution, holding the rest fixed.

\begin{itemize}
    \item<1-> \textbf{Topic assignments:} The assignments are conditionally independent given the topic parameters and proportions,
    \begin{align}
        p(z_{n,\ell} \mid \mbx_n, \{\mbeta_k\}_{k=1}^K, \mbpi_n) &\propto \pi_{n, z_{n, \ell}} p(x_{n, \ell} \mid \mbeta_{z_{n, \ell}})
    \end{align}
    
    \item<2-> \textbf{Topic proportions: } Let $N_{n,k} = \sum_{\ell=1}^L \bbI[z_{n,\ell} = k]$ denote the number of words in document $n$ assigned to topic $k$. Then,
    \begin{align}
        p(\mbpi_n \mid \mbalpha, \mbz_n) &\propto \distDirichlet(\mbpi_n \mid \mbalpha) \prod_{k=1}^K \pi_{n,k}^{N_{n,k}} = \distDirichlet([\alpha_1 + N_{n,1}, \ldots, \alpha_K + N_{n,K}]).
    \end{align}
    
    \item<3-> \textbf{Topic parameters: } Let $N_{k,v} = \sum_{n=1}^N \sum_{\ell=1}^L \bbI[z_{n,\ell}=k] \bbI[x_{n,\ell}=v]$ denote the number of times word~$v$ was assigned to topic $k$ across all documents. Then,
\begin{align}
    p(\mbeta_k \mid \{\mbz_n, \mbx_n\}, \mbphi) &\propto
    \distDirichlet(\mbeta_k \mid \mbphi) \prod_{v=1}^V \eta_{k,v}^{N_{k,v}} = \distDirichlet([\phi_1 + N_{k,v}, \ldots, \phi_V + N_{k,V}]).
\end{align}

\end{itemize}
\end{frame}

\begin{frame}{Why does LDA produce sharp topics?}
Consider the log posterior as a function of $\mbeta_k$ and $\mbpi_n$. From eq.~\ref{eq:lda_joint}, it is,
\begin{align}
    \sum_{k=1}^K \log p(\mbeta_k \mid \mbphi) + \sum_{n=1}^N \log p(\mbpi_n \mid \mbalpha) + \sum_{n=1}^N \sum_{\ell=1}^L \left( \log \pi_{n, z_{n, \ell}} + \log \eta_{z_{n, \ell}, x_{n, \ell}} \right)
\end{align}
The double sum over $n$ and $\ell$ dominates this expression.

It encourages topic proportions and topic probabilities to both be large, but recall that both $\mbpi_n$ and $\mbeta_k$ are constrained to the simplex.

For $\log \pi_{n,z_{n,\ell}}$ to be large, the posterior should assign all words to as few topics as possible. 

For $\log \eta_{z_{n,\ell}, x_{n, \ell}}$ to be large, the topics should put high probability on as few words as possible.

These goals are at odds. The LDA posterior balances these goals to find topics with sharply co-occuring words.

\end{frame}

% \begin{frame}{From words to word counts}

% So far we've treated documents as a sequence of discrete words, but note that all we really needed were the word counts $N_{n,v} = \sum_{\ell=1}^L \bbI[x_{n,\ell} = v]$
    
% \end{frame}

\section{Algorithm: CAVI}
\label{sec:cavi}

\begin{frame}{Lap 5: Mixed Membership Models and Variational Inference}
\begin{itemize}
    \item \hyperref[sec:models]{Model: Mixed Membership Models}
    \item \hyperref[sec:cavi]{\textbf{Algorithm: Coordinate Ascent Variational Inference (CAVI)}}
\end{itemize}
\end{frame}

\begin{frame}{Taking stock}
    
We've covered a number of posterior inference algorithms thus far:
\begin{itemize}
    \item \textbf{Exact inference:} for simple models (e.g. conjugate exponential family models) where the posterior is available in closed form.
    
    \item \textbf{Laplace approximation:} approximate the posterior with a Gaussian centered on the posterior mode that matches the local curvature. This works well for sharp, unimodal posteriors over continuous latent variables.
    
    \item \textbf{Metropolis-Hastings: } a very general MCMC algorithm to sample the posterior, and the building block for many other MCMC techniques.
    
    \item \textbf{Hamiltonian Monte Carlo: } an MCMC algorithm to draw samples from the posterior by leveraging gradients of the log joint probability. This works well for more general posteriors over continuous variables.
    
    \item \textbf{Gibbs sampling:} an MCMC algorithm that iteratively samples conditional distributions for one variable at a time. This works well for conditionally conjugate models with weak correlations.
\end{itemize}
\end{frame}

\begin{frame}{Variational inference}

We liked the Laplace approximation because they were deterministic and often very efficient to compute, but they were biased for non-Gaussian posteriors.

MCMC methods are asymptotically unbiased (though for finite samples there is a transient bias that shrinks as $O(S^{-1})$. The real issue is variance: it only shrinks as $O(S^{-1/2})$.

\textbf{Motivation: } With finite computation, can we get better posterior estimates by trading asymptotic bias for smaller variance? 

\textbf{Idea: } approximate the posterior by with a simple, parametric form (though not strictly a Gaussian on the mode!). Optimize to find the approximation that is as ``close'' as possible to the posterior.
    
\end{frame}

\begin{frame}{Notation}
Let,
\begin{itemize}
    \item $\mbtheta \in \reals^P$ denote the collection of latent variables and parameters we wish to infer. \\
    In LDA, $\mbtheta = \{\mbeta_k\}_{k=1}^K, \{\mbpi_n, \mbz_n\}_{n=1}^N$.
    
    \item $p(\mbtheta \mid \mbx)$ denote the posterior distribution we want to approximate.
    
    \item $q(\mbtheta; \mblambda)$ denote a parametric \textit{variational approximation} to the posterior where...
    
    \item $\mblambda$ denotes the \textit{variational parameters} that we will optimize.
    
    \item $D(q \, \| \, p)$ denote a \textit{divergence measure} that takes in two distributions $q$ and $p$ and returns a measure of how similar they are.
\end{itemize}
\end{frame}

\begin{frame}{A view of variational inference}
\centering
    \includegraphics[width=.5\textwidth]{figures/lap5/vi.png}
\end{frame}

\begin{frame}{Key questions}
\begin{itemize}
    \item \textit{What parametric family should we use? }
    \begin{itemize}
        \item This lecture: the \textbf{mean-field family}.
    \end{itemize}
    
    \item \textit{How should we measure closeness? }
    \begin{itemize}
        \item This lecture: the \textbf{Kullback-Leibler (KL)} divergence.
    \end{itemize}
    
    \item \textit{How do we find the closest distribution in that family?}
    \begin{itemize}
        \item This lecture: \textbf{coordinate ascent.}
    \end{itemize}
\end{itemize}

These choices are what \citet{Blei2017-yc} call \textbf{coordinate ascent variational inference}~CAVI).
\end{frame}

\begin{frame}{The mean-field family}
The \textit{mean-field family} gets its name from statistical mechanics. It treats each latent variable and parameter as independent with its own variational parameter,
\begin{align}
    q(\mbtheta; \mblambda) &= \prod_{j=1}^P q(\theta_j; \lambda_j).
\end{align}

For example, in LDA the mean field approximation treats each topic, topic proportion, and topic assignment as independent,
\begin{align}
    q(\{\mbeta_k\}_{k=1}^K, \{\mbpi_n, \mbz_n\}_{n=1}^N; \mblambda) &= \prod_{k=1}^K q(\mbeta_k; \mblambda_k^{(\mbeta)}) 
    \prod_{n=1}^N q(\mbpi_n; \mblambda_n^{(\mbpi)}) 
    \prod_{n=1}^N \prod_{\ell=1}^L q(z_{n, \ell}; \mblambda_{n, \ell}^{(z)})
\end{align}

\textbf{Question: } Is this a good approximation to the posterior? 

\end{frame}

\begin{frame}{The Kullback-Leibler (KL) divergence}
The KL divergence is a measure of closeness between two distributions. It is defined as,
\begin{align}
    \KL{q(\mbtheta; \mblambda)}{p(\mbtheta \mid \mbx} &= \E_{q(\mbtheta; \mblambda)} \left[ \log \frac{q(\mbtheta; \mblambda)}{p(\mbtheta \mid \mbx)} \right] \\
    &= \E_{q(\mbtheta; \mblambda)} \left[ \log q(\mbtheta; \mblambda) \right] 
    - \E_{q(\mbtheta; \mblambda)} \left[ \log p(\mbtheta \mid \mbx) \right] 
    % \\
    % &= \underbrace{\bbH[q(\mbtheta; \mblambda), p(\mbtheta \mid \mbx)]}_{\text{cross entropy}} - \underbrace{\bbH[q(\mbtheta; \mblambda)]}_{\text{entropy}}
\end{align}

It has some nice properties:
\begin{itemize}
    \item It is non-negative.
    
    \item It is zero iff $q(\mbtheta; \mblambda) \equiv p(\mbtheta \mid \mbx)$.
    
    \item It is defined in terms of expectations wrt $q$.
\end{itemize}

But it's also a bit weird... 
\begin{itemize}
    \item It's asymmetric ($\KL{q}{p} \neq \KL{p}{q}$).
\end{itemize}
\end{frame}

\begin{frame}{The evidence lower bound (ELBO)}
    More concerning, the KL divergence involves the posterior $p(\mbtheta \mid \mbx)$, which we cannot compute!
    
    But notice that...
    \begin{align}
        \KL{q(\mbtheta; \mblambda)}{p(\mbtheta \mid \mbx}
        &= \E_{q(\mbtheta; \mblambda)} \left[ \log q(\mbtheta; \mblambda) \right] 
        - \E_{q(\mbtheta; \mblambda)} \left[ \log p(\mbtheta \mid \mbx) \right] \\
        &= \E_{q(\mbtheta; \mblambda)} \left[ \log q(\mbtheta; \mblambda) \right] 
        - \E_{q(\mbtheta; \mblambda)} \left[ \log p(\mbtheta, \mbx) \right] 
        + \E_{q(\mbtheta; \mblambda)}\left[ \log p(\mbx)\right] \\
        &= \underbrace{\E_{q(\mbtheta; \mblambda)} \left[ \log q(\mbtheta; \mblambda) \right] 
        - \E_{q(\mbtheta; \mblambda)} \left[ \log p(\mbtheta, \mbx) \right]}_{\text{negative ELBO}, -\cL(\mblambda)}
        + \underbrace{\log p(\mbx)}_{\text{evidence}}
    \end{align}
The first term involves the log joint, which we can compute, and the last term is independent of the variational parameters! 

Rearranging, we see that $\cL(\mblambda)$ is a lower bound on the marginal likelihood, aka the evidence,
\begin{align}
    \cL(\mblambda) &= \log p(\mbx) - \KL{q(\mbtheta; \mblambda)}{p(\mbtheta \mid \mbx} 
    \leq \log p(\mbx).
\end{align}
That's why we call it the \textbf{evidence lower bound (ELBO)}.
\end{frame}

\begin{frame}{Viewer discretion advised...}
    
    \centering
    % \includemedia[
    %     width=0.75\linewidth,height=0.3375\linewidth, % 16:9
    %     activate=pageopen,
    %     flashvars={
    %     modestbranding=1 % no YT logo in control bar
    %     &autohide=1 % controlbar autohide
    %     &showinfo=0 % no title and other info before start
    %     &rel=0 % no related videos after end
    %     }
    %     ]{}{https://www.youtube.com/watch?v=jugUBL4rEIM}
    \url{https://www.youtube.com/watch?v=jugUBL4rEIM}
    
\end{frame}

\begin{frame}{Optimizing the ELBO with coordinate ascent}
We want to find the variational parameters $\mblambda$ that minimize the KL divergence or, equivalently, maximize the ELBO.

For the mean-field family, we can typically do this via \textbf{coordinate ascent}. 

Consider optimizing the parameters for one factor~$q(\theta_j; \lambda_j)$. As a function of $\lambda_j$, the ELBO is,
\begin{align}
    \cL(\mblambda) 
    &= 
    \E_{q(\theta_j; \lambda_j)}\left[ \E_{q(\mbtheta_{\neg j}; \mblambda_{\neg j})} \left[ \log p(\mbtheta, \mbx) \right] \right] -
    \E_{q(\theta_j; \lambda_j)}[\log q(\theta_j; \lambda_j)] + c \\
    &= 
    \E_{q(\theta_j; \lambda_j)}\left[ \E_{q(\mbtheta_{\neg j}; \mblambda_{\neg j})} \left[ \log p(\theta_j \mid \mbtheta_{\neg_j}, \mbx) \right] \right] -
    \E_{q(\theta_j; \lambda_j)}[\log q(\theta_j; \lambda_j)] + c' \\
    &= -\KL{q(\theta_j; \lambda_j)}{\tilde{p}(\theta_j)} + c''
\end{align}
where 
\begin{align}
    \tilde{p}(\theta_j) &\propto
    \exp \left\{ \E_{q(\mbtheta_{\neg j}; \mblambda_{\neg j})} \left[ \log p(\theta_j \mid \mbtheta_{\neg j}, \mbx) \right] \right\}
\end{align}
The ELBO is maximized wrt $\lambda_j$ when this KL is minimized; i.e. when $q(\theta_j ; \lambda_j) = \tilde{p}(\theta_j)$, the exponentiated expected log conditional probability, holding all other factors fixed.
\end{frame}

\begin{frame}{Next time}
    
    \begin{itemize}
        \item Coordinate ascent variational inference for LDA
        \item Word count formulation
        \item Evaluating topic models and choosing $K$
        \item Stochastic variational inference
    \end{itemize}
\end{frame}

\begin{frame}[t,allowframebreaks]
        \frametitle{References}
        \bibliographystyle{unsrtnat}
        \bibliography{refs.bib}
\end{frame}

\end{document}