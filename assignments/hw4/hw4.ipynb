{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "# HW4: Bayesian Mixture Models\n",
    "\n",
    "\n",
    "**STATS271/371: Applied Bayesian Statistics**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "---\n",
    "\n",
    "**Name:** _Your name here_\n",
    "\n",
    "**Names of any collaborators:** _Names here_\n",
    "\n",
    "*Due: 11:59pm Monday, May 3, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "\n",
    "In this homework assignment we will investigate image segmentation ---specifically, separating the background from the foreground of the image. To do so, you'll learn Bayesian mixtures of Gaussians using Gibbs sampling.\n",
    "\n",
    "### Background: Image Segmentation\n",
    "The figure below shows the original input image and the resulting segmentations into background and foreground. By the end of this assignment, you will have implemented the algorithm to achieve this segmentation.\n",
    "\n",
    "Reference on image segmentation: https://en.wikipedia.org/wiki/Image_segmentation\n",
    "\n",
    "![Fox](images/fox_seg.png \"Segmentation of fox image\")\n",
    "\n",
    "\n",
    "### Background: Non-Bayesian Mixtures\n",
    "\n",
    "To set the stage, we begin with a straightforward finite mixture model to cluster the pixels (with the number of clusters $K = 2$ in our image segmentation problem). The likelihood of the model is defined as a mixture of Gaussian distributions.\n",
    "\n",
    "$$\n",
    "p(x_n \\mid z_n, \\{\\mu_k, \\Sigma_k\\}_{k=1}^K) = \\mathcal{N}(x_n; \\mu_{z_n}, \\Sigma_{z_n})\n",
    "$$\n",
    "\n",
    "where $x_n \\in \\mathbb{R}^D$ is distributed according to a Gaussian distribution with the specified mean, $\\mu_k$, and covariance, $\\Sigma_k$, for its correspondign cluster $z_n = k$. We will represent the images as a set of $N$ pixels, $\\{x_n\\}_{n=1}^N$, each in $D=3$ dimensional space, since there are three color channels (red, green, and blue).\n",
    "\n",
    "### Bayesian Mixture\n",
    "\n",
    "We specify the following priors on $\\mu_k$, $\\Sigma_k$, and $\\pi$.\n",
    "- Assume a normal-inverse-Wishart prior prior for each cluster mean and covariance.\n",
    "\\begin{align} \n",
    "p(\\mu_k, \\Sigma_k) &= \\mathrm{IW}(\\Sigma_k \\mid \\Psi_0, \\nu_0) \\, \\mathcal{N}(\\mu_k \\mid m_0, \\kappa_0^{-1} \\Sigma_k)\n",
    "\\end{align}\n",
    "Here $\\Psi_0, \\nu_0, m_0, \\kappa_0$ are hyper-parameters.\n",
    "\n",
    "- We give a symmetric Dirichlet distribution prior to the mixing proportions, $\\pi$:\n",
    "$$ \n",
    "p(\\pi \\mid \\alpha) = \\text{Dirichlet}(\\alpha)\n",
    "$$\n",
    "with the hyper-parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 [math]: posterior calculations\n",
    "In this problem, you will derive the conditional posterior distributions of the various model parameters.\n",
    "\n",
    "These are related to the joint posterior distribution:\n",
    "$$\n",
    "p(\\pi, \\{\\mu_k, \\Sigma_k\\}_{k=1}^K, \\{z_n\\}_{n=1}^N \\mid \\{x_n\\}_{n=1}^N, \\alpha, \\nu_0, \\Psi_0, m_0, \\kappa_0) \n",
    "\\propto p(\\pi \\mid \\alpha) \\prod_{k=1}^K p(\\mu_k, \\Sigma_k \\mid \\nu_0, \\Psi_0, m_0, \\kappa_0) \\prod_{n=1}^N p(z_n \\mid \\pi) p(x_n \\mid \\mu_{z_n}, \\Sigma_{z_n})\n",
    "$$\n",
    "\n",
    "\n",
    "You will need these derivations to be correct for the implementation in Problem 2 to be correct, so we highly recommend taking the time to double check them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNEAFfuV78xh"
   },
   "source": [
    "### (a) Derive the complete conditional for the parameters $\\mu_k, \\Sigma_k$.\n",
    "It should be a normal-inverse-Wishart distribution, like the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNEAFfuV78xh"
   },
   "source": [
    "The conditional distributions for $z_n$ and $\\pi$ are the same as shown in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK9HU0boZxD0"
   },
   "source": [
    "## Problem 2: Gibbs Sampler\n",
    "We have provided starter code below.\n",
    "- First, you need to fill it with your own implementation of the Gibbs sampling algorithm. You may not rely on external implementations such as those offered by Tensorflow or scikit-learn.\n",
    "- Then, you will test your code on a simple example.\n",
    "\n",
    "### (a) Implement the Gibbs Sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(X, K=2, \n",
    "          n_iter=100, \n",
    "          alpha=np.ones(2), \n",
    "          m0=np.zeros(3), \n",
    "          kappa0=1.0, \n",
    "          nu0=1.0, \n",
    "          Psi0=np.eye(3)):\n",
    "    \"\"\"\n",
    "    Run the Gibbs samlper for a mixture of Gaussians with a NIW prior.\n",
    "    \n",
    "    Input:\n",
    "    - X: Matrix of size (N, D). Each row of X stores one data point\n",
    "    - K: the desired number of clusters in the model. Default: 2\n",
    "    - n_iter: number of iterations of Gibbs sampling. Default: 100\n",
    "    - alpha: hyperparameter of the Dirichlet prior on \\pi.\n",
    "    - m0, kappa0, nu0, Psi0, hyperparameters of normal-inverse-Wishart prior.\n",
    "        \n",
    "    Returns: \n",
    "    - log joint probability for each iteration\n",
    "    - samples of parameters and assignments over iterations\n",
    "    \n",
    "    You will use these values later on for convergence diagnostics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement the Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) test your implementation \n",
    "Test your example on a synthetic data set.\n",
    "\n",
    "For example, the ground truth could be two clusters, with means [5,5,5] and [8,8,8] respectively. You could generate $50$ points in each cluster. \n",
    "\n",
    "Whichever example you choose, be sure to specify it and show that your implementation roughly recovers the ground truth by displaying the cluster means/covariances and trace plots of the log joint probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 : Perform image segmentation\n",
    "Now that you have implemented the Gibbs Sampler, you are ready to perform image segmentation!\n",
    "First, we define helper code to load the images and save the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    # Read in the image, dropping the alpha channel\n",
    "    image = plt.imread(filename + \".png\")[:, :, :3]\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # get height, width and number of channels\n",
    "    H, W, C = image.shape\n",
    "    X = image.copy().astype(float)\n",
    "\n",
    "    # reshape into pixels, each has 3 channels (RGB)\n",
    "    X = X.reshape((H * W, C)) \n",
    "    return image, X\n",
    "\n",
    "def save_segmentation(image, assignments, filename=None):\n",
    "    fig, axs = plt.subplots(1, K + 1, figsize=(4 * (K + 1), 4))\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_axis_off()\n",
    "    axs[0].set_title(\"original image\")\n",
    "    \n",
    "    for k in range(K):\n",
    "        im = image.copy()\n",
    "        im[assignments != k] = np.nan\n",
    "        axs[k+1].imshow(im)\n",
    "        axs[k+1].set_axis_off()\n",
    "        axs[k+1].set_title(\"component {}\".format(k))\n",
    "    \n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Gibbs sampler\n",
    "Run the Gibbs sampler on the 4 provided images: \"cow\", \"fox\", \"owl\", \"zebra\". The hyper-parameters values should be the default ones from the Gibbs function.\n",
    "\n",
    "Each sample of $\\{z_n\\}_{n=1}^N$ is a segmentation. To aggregate them into one sample, you could assign each pixel to the component it was most often attributed to in your samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUmU46t-78xj"
   },
   "source": [
    "## Problem 4: Diagnostics\n",
    "### (a)\n",
    "Make trace plots of the log joint probability and posterior marginals of the cluster means (e.g. as histograms for R, G, and B weights.)\n",
    "\n",
    "### (b)\n",
    "(Approximately) how many iterations are needed for convergence? Does this depend on the input image and/or initialization of the model parameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Extensions\n",
    "\n",
    "### (a) Sample the posterior predictive distribution to get a new image\n",
    "Plot the resulting images.  \n",
    "\n",
    "### (b) Improvements to the model\n",
    "The generated images should not look realistic. Explain why that is the case: suggest improvements to the generative model that would make for more realistic samples.\n",
    "You do not need to implement the change though.\n",
    "\n",
    "### (c) Label switching\n",
    "What could go wrong with the proposed method of deriving segmentations in problem 3? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OnB5kf-k7B0"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf hw4_yourname.ipynb\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
