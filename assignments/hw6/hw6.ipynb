{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "# HW6: Variational Autoencoders\n",
    "\n",
    "\n",
    "**STATS271/371: Applied Bayesian Statistics**\n",
    "\n",
    "_Stanford University. Spring, 2021._\n",
    "\n",
    "---\n",
    "\n",
    "**Name:** _Your name here_\n",
    "\n",
    "**Names of any collaborators:** _Names here_\n",
    "\n",
    "*Due: 11:59pm Monday, May 17, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "\n",
    "Our goal is to learn the parameters $\\Theta$ of a distribution over data, $p(x_n \\mid \\Theta)$. We assume a latent variable model of the following form:\n",
    "$$\n",
    "p(x_n \\mid \\Theta) = \\int p(x_n \\mid z_n, \\Theta) \\, p(z_n) \\mathrm{d} z_n\n",
    "$$\n",
    "The prior $p(z_n)$ can be very simple (e.g. a standard Gaussian distribution) as long as the likelihood, $p(x_n \\mid z_n, \\Theta)$, is sufficiently flexible. Then the latent variable model can capture very complex data distributions.\n",
    "\n",
    "Variational autoencoders (VAEs) are one way of training latent variable models like the one above. We'll build a very simple VAE in this homework assignment and apply it to a standard image dataset, the MNIST dataset of handwritten digits.\n",
    "\n",
    "Assume the following functional forms for the latent variable model above,\n",
    "\\begin{align}\n",
    "p(z_n) &= \\mathcal{N}(z_n \\mid 0, I) \\\\\n",
    "p(x_n \\mid z_n, \\Theta) &= \\mathcal{N}(f(z_n, w), \\Sigma)\n",
    "\\end{align}\n",
    "where $f$ is a nonlinear mapping from latent variables $z$ to expected observations $\\mathbb{E}[x_n \\mid z_n] = f(z_n, w)$. The full set of generative model parameters are $\\Theta = (w, \\Sigma)$. \n",
    "\n",
    "We'll use variational expectation-maximization (vEM) to learn the model parameters. This entails an inner inference step (the variational E-step) to approximate the posterior\n",
    "\\begin{align}\n",
    "q(z_n; \\lambda_n) &\\approx p(z_n \\mid x_n, \\Theta).\n",
    "\\end{align}\n",
    "Optimizing these variational parameters $\\lambda_n$ for each data point can be very time consuming, involving many iterations of gradient ascent for each variational E-step.\n",
    "\n",
    "The key insight of VAEs is that our time might be better spent optimizing the model parameters instead, and that we can get by with a worse approximation to the posterior if it allows us more updates of $\\Theta$. To that end, VAEs simultaneously train an *inference network* to quickly map data points $x_n$ to variational parameters $\\lambda_n$. Concretely, VAEs learn a function,\n",
    "\\begin{align}\n",
    "\\lambda_n &= g(x_n; \\phi),\n",
    "\\end{align}\n",
    "The variational parameters $\\phi$ are shared by all datapoints, thereby *amortizing* the cost of inference across examples. Under this formulation, we will simply write,\n",
    "\\begin{align}\n",
    "q(z_n; \\lambda_n) &= q(z_n; g(x_n; \\phi)) \\triangleq q(z_n; x_n, \\phi).\n",
    "\\end{align}\n",
    "\n",
    "To train a variational autoencoder, we perform stochastic gradient ascent on the ELBO $\\mathcal{L}(\\Theta, \\phi)$ with respect to both $\\Theta$ and $\\phi$ using Monte Carlo estimates of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "## Problem 1: Math\n",
    "\n",
    "Consider a dataset $\\{x_n\\}_{n=1}^N$ with $x_n \\in \\mathbb{R}^D$ and assume continuous latent variables $z_n \\in \\mathbb{R}^P$ with a standard normal prior. We'll assume a variational approximation to the posterior on $z_n \\in \\mathbb{R}^P$ of the form,\n",
    "\\begin{align}\n",
    "q(z_n; x_n, \\phi) &= \\mathcal{N}(z_n; \\mu_n, \\mathrm{diag}(\\sigma_n^2)),\n",
    "\\end{align}\n",
    "where $\\mu_n \\in \\mathbb{R}^P$ and $\\sigma_n^2 \\in \\mathbb{R}_+^P$ are the variational parameters output by the inference network $g(x_n; \\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 1a: Write a Monte Carlo estimator for the ELBO \n",
    "Use random mini-batches of data to write a Monte Carlo estimate of the ELBO,\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\Theta, \\phi) &\\approx \\ldots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 1b: Write a Monte Carlo estimate of the gradient wrt $\\Theta$\n",
    "Use random mini-batches of data to write a Monte Carlo estimate of the gradient,\n",
    "\\begin{align}\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta, \\phi) &\\approx \\ldots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 1c: Derive the KL divergence between two Gaussians\n",
    "Derive the KL divergence between two multivariate normal distributions,\n",
    "\\begin{align}\n",
    "\\mathrm{KL}\\big(\\mathcal{N}(\\mu_1, \\Sigma_1) \\, \\| \\, \\mathcal{N}(\\mu_2, \\Sigma_2) \\big) &= \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 1d: Write a Monte Carlo estimate of the gradient wrt $\\phi$\n",
    "Use reparameterization gradients and random mini-batches of data to write a Monte Carlo estimate of the gradient,\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathcal{L}(\\Theta, \\phi) &\\approx \\ldots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "## Problem 2: Code\n",
    "In this problem, you will implement a simple VAE model and train it on the MNIST handwritten digits dataset. The inputs 28x28 pixel images, which are flattened to into vectors of dimension $D=784$.  Let both $p(x_n \\mid z_n, \\Theta)$ and $q(z_n ; x_n, \\phi)$ be parametrized by neural networks with one hidden layer that consists of $512$ ReLU neurons and let the dimensionality of the latent space be $P=2$. The weight matrices between the layers should be initialized randomly by sampling from $\\mathcal{N}(0, 0.01)$ and the biases should be initially set to zeros. Since the $x_n$'s are continuous but standardized to lie in $[0,1]$, the output layer of the generative network $f(z_n, w)$ should have sigmoidal nonlinearities.\n",
    "\n",
    "The variational distribution, $q(z_n; x_n, \\phi)$, is a diagonal Gaussian distribution, as in Problem 1. The inference network should output a mean vector and a *non-negative* vector of variances for each latent dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 2a: Build the VAE\n",
    "Build the VAE described above. There's no \"right\" way to organize your code, and different deep learning frameworks encourage different types of implementations. In Python, I like to use classes to encapsulate the parameters of the generative and inference networks (i.e. $\\Theta$ and $\\phi$). The class would expose automatically differentiable functions like `infer`, `generate`, and `compute_elbo` to map data points to posterior parameters, compute the mean of the image given a latent sample, and evaluate a Monte Carlo estimate of the ELBO for a mini-batch of data, respectively. Then you can use stochastic gradient ascent to maximize the ELBO with respect to the parameters.\n",
    "\n",
    "#### Note on implementation:\n",
    "- You are free to use any programming language for your implementation.\n",
    "- We recommend you additionally use a library with support that allows you to perform automatic reverse-mode differentiation which will simplify model development. Both TensorFlow or PyTorch, e.g., have implemented distributions that make it easy to implement reparameterization gradients.\n",
    "- You are *not* allowed to use any libraries that provide some sort of pre-made implementations of the variational autoencoders. That is, one line implementations like `vae = VAE(...)` are not in the spirit of this assignment.\n",
    "- For the optimization, we recommend you use one of the popular algorithms such as Adagrad [1] or Adam [2].\n",
    "\n",
    "\n",
    "#### Note on the Honor Code:\n",
    "- There are many examples freely available on the internet of similar implementations. If you follow any such sources, you must clearly cite them in your submission. \n",
    "- You need to implement the models and the training algorithms using standard libraries (including TensorFlow, PyTorch, Keras, etc.) yourself.\n",
    "\n",
    "#### References\n",
    "- [1] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121â€“2159, 2011.\n",
    "- [2] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: Train the VAE with stochastic gradient ascent on the ELBO\n",
    "\n",
    "Train and evaluate your models on the MNIST handwritten digits dataset. The dataset can be downloaded directly from [here](http://yann.lecun.com/exdb/mnist/). Alternatively, many deep learning libraries have utilities to download the dataset into their desired format. (E.g. [PyTorch](https://pytorch.org/vision/stable/datasets.html#mnist), [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data), and [TensorFlow for R](https://tensorflow.rstudio.com/guide/keras/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 3a: Sample from the VAE\n",
    "Visualize a random sample of $100$ MNIST digits on $10 \\times 10$ tile grid (i.e., $10$ rows, $10$ digits per row).\n",
    "Using your trained models, sample and visualize $100$ digits from each of them in the same manner. To do this, sample $100$ random $z$, then apply the generator network, $p(x_n \\mid z_n)$, to produce digit samples. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 3b: Visualize the manifold of digits\n",
    "Since we have specifically chosen the latent space to be 2-dimensional, now you can easily visualize the learned latent manifold of digits:\n",
    "- Using your pre-trained recognition networks, transform images from the test set to the latent space. Visualize the points in the latent space as a scatter plot, where colors of the points should correspond to the labels of the digits.\n",
    "- From the previous point, determine the min and max values of $z_1$ and $z_2$. Create a $20 \\times 20$ grid that corresponds to $(z_1, z_2)$ values between the min and max. For each $z = (z_1, z_2)$, generate and visualize digits using each of your trained models, and plot each set on a $20 \\times 20$ tile grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "## Problem 4: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 4a: Laplace prior on the latents\n",
    "Suppose we instead used a [Laplace prior](https://en.wikipedia.org/wiki/Laplace_distribution) $z_n \\sim \\mathrm{Lap}(0, \\tau)$ with density\n",
    "\\begin{align}\n",
    "p(z_n) &= \\frac{1}{2\\tau}\\exp \\left\\{-\\frac{|z_n|}{\\tau} \\right\\}.\n",
    "\\end{align}\n",
    "Propose a simple reparametrization of the Laplace distribution $z_n = r(\\epsilon_n, \\tau)$ with $\\epsilon_n \\sim p(\\epsilon)$ for some function $r$ and distribution $p$, suitable for training a VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "### Problem 4b: Discrete latent variables\n",
    "The present model uses continuous latent variables. Where did we use this assumption and what would have to change if we used discrete $z_n$'s instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf hw6_yourname.ipynb\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of STATS271 HW1: Bayesian Linear Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
